name: Learning Scraper

on:
  repository_dispatch:
    types: [scrape_request]
  workflow_dispatch:
    inputs:
      url:
        description: 'URL to scrape'
        required: true
        default: 'https://example.com'
      priority:
        description: 'Task priority'
        required: false
        default: 'normal'

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        # Removed cache: 'pip' to avoid requirements.txt issues
    
    - name: Install dependencies directly
      run: |
        pip install --upgrade pip
        pip install playwright==1.40.0 beautifulsoup4==4.12.2 httpx==0.25.2 python-dotenv==1.0.0 requests==2.31.0
    
    - name: Install Playwright browsers
      run: |
        playwright install chromium
        playwright install-deps
    
    - name: Create directories
      run: |
        mkdir -p screenshots
        mkdir -p logs
    
    - name: Debug environment
      run: |
        echo "üîç Environment check:"
        echo "Python version: $(python --version)"
        echo "Current directory: $(pwd)"
        echo "Files in repo: $(ls -la)"
        echo "TARGET_URL: ${{ github.event.client_payload.url || github.event.inputs.url }}"
    
    - name: Run learning scraper
      env:
        SYNAPSE_HUB_URL: ${{ secrets.SYNAPSE_HUB_URL }}
        SENSORY_API_KEY: ${{ secrets.SENSORY_API_KEY }}
        TARGET_URL: ${{ github.event.client_payload.url || github.event.inputs.url }}
        PRIORITY: ${{ github.event.client_payload.priority || github.event.inputs.priority }}
      run: |
        echo "üß† Starting Sensory Neurons..."
        echo "Target URL: $TARGET_URL"
        echo "Hub URL: $SYNAPSE_HUB_URL"
        python scraper.py
        echo "‚úÖ Scraper completed"
    
    - name: Check created files
      if: always()
      run: |
        echo "üìÅ Checking created files:"
        ls -la
        ls -la screenshots/ || echo "No screenshots directory"
        cat results.json || echo "No results.json file"
    
    - name: Upload results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: scraping-results-${{ github.run_id }}
        path: |
          screenshots/
          results.json
        retention-days: 3
        if-no-files-found: warn
